{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Notebook:\n",
    "\n",
    "Currently Handles only case of single task - can be adapted for mutlitask output by summing across losses and taking a step back in that way: See --> https://towardsdatascience.com/tuning-a-multi-task-fate-grand-order-trained-pytorch-network-152cfda2e086"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Requirements:**\n",
    "\n",
    "1) Pytorch and all subclasses (eg., nn and F) <br>\n",
    "2) DontGetSNPpyWithMe.py in same folder as this notebook<br>\n",
    "3) EarlyStop.py in same folder as this notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import necessary packages:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from DontGetSNPpyWithMe import * #The neural net\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from EarlyStop import * #Early Stopping object\n",
    "from data_loader import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define a Validation Splitting Function (maybe handle externally by randomly sampling in a phenotype balanced way so can just read in and go here), and read in training, validation, and test (openSNP) data:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ValSplit():\n",
    "    return None\n",
    "\n",
    "training = torch.randint(0,8, size = (5, 500000)).float() #DataLoader(...)\n",
    "trainingLabels = torch.randint(0,3, size= (5,1)) #random numbers between 0 and 2 --> simulate 3 class problem\n",
    "val = None #DataLoader(...)\n",
    "test = None #DataLoader(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define variables that can be easily changed for model instantiation later in one place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = get_loader(genotype_file=\"../data/2020_05_26/train_set.csv\",\n",
    "                          phenotype_file=\"../data/2020_05_26/train_labels.csv\",\n",
    "                          batch_size=32,\n",
    "                          shuffle=False,\n",
    "                          num_workers=4)\n",
    "val_loader = get_loader(genotype_file=\"../data/2020_05_26/val_set.csv\",\n",
    "                          phenotype_file=\"../data/2020_05_26/val_labels.csv\",\n",
    "                          batch_size=32,\n",
    "                          shuffle=False,\n",
    "                          num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchSize = 5\n",
    "numSNPs= 69258  # 500000\n",
    "numLayers = 5\n",
    "layerWidths = [512, 512,256,128,64]\n",
    "dropout = 0.3\n",
    "multitaskOutputs = [3]\n",
    "learningRate = 5e-3\n",
    "epochs = 100\n",
    "patience = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the model, loss function, and optimizer using the above parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derekZoolander = DiddyKongRacing([batchSize,numSNPs], numLayers, layerWidths, dropout, multitaskOutputs)\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.Adam(derekZoolander.parameters(), lr=learningRate) # can add in regularization w/ wd as well later: weight_decay=1e-4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Move the model to gpu if available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pleaseBeGpu = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(pleaseBeGpu)\n",
    "derekZoolander = derekZoolander.to(pleaseBeGpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a Function to calc accuracy on a batch since the model returns raw outputs and not softmax!:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input of 2 tensors --> make sure labels is a tensor of size == batch size\n",
    "#output: Total number of matches (accuracy can be returned by removing the comment # below); \n",
    "# this was done to get accuracy on an epoch basis where can be weighted by total number of samples as opposed to a per batch basis\n",
    "def AccuracyCalc(raw, labels): \n",
    "    print(F.softmax(raw, dim = 1))\n",
    "    payMeForMyPredictions = torch.argmax(F.softmax(raw, dim = 1), dim = 1)\n",
    "    print(payMeForMyPredictions)\n",
    "    return sum(payMeForMyPredictions.eq(labels)).item() #/len(lables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a validation (and Test) function (also base case of evaluation for epoch 0): \n",
    "\n",
    "Can mute print statements later when running everything especially with test set and when plotting is integrated but for now they are there to ensure things intuitively are working and can get a sense model is improving in real time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs: 1) model (derekZoolander) 2) current epoch number 3) validation dataloader 4) Loss function 5) device\n",
    "#outputs: 1) accuracy 2) loss\n",
    "\n",
    "def Validate(model, epochNum, data = val, lossFn = criterion, device = pleaseBeGpu):\n",
    "    model.eval() #shut off batch norm and dropout\n",
    "    # numBatches = 0 in case want to know how many batches iterating through\n",
    "    totalSamples = 0\n",
    "    totalLoss = 0\n",
    "    for i, (snpBatch, phenotypeBatch) in enumerate(data):            \n",
    "        totalSamples += len(phenotypeBatch) #running total of number of samples\n",
    "\n",
    "        #move labels to gpu (hopefully)\n",
    "        snpBatch = snpBatch.to(device)\n",
    "        phenotypeBatch = phenotypeBatch.to(device)\n",
    "\n",
    "        #Run it:\n",
    "        output = model(snpBatch)\n",
    "        loss = lossFn(output[0], phenotypeBatch) #recall output returns a list to handle multi-task learning so here the prediction is output[0]\n",
    "\n",
    "        #Calculate accuracy:\n",
    "        accuracy += AccuracyCalc(output[0], phenotypeBatch)\n",
    "        totalLoss += loss.item()\n",
    "        \n",
    "    epochAcc = numerator/(totalSamples)\n",
    "    print(\"Validation accuracy at epoch {} is: {} .\".format(epochNum,epochAcc))\n",
    "    return epochAcc, totalLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a training f(x):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inputs: 1) model 2) Number of epochs want to train for 3) training dataloader 4) loss function \n",
    "# 5) the device available (cpu or gpu/cuda --> hopefully it is the latter)\n",
    "\n",
    "#Outputs: 4 lists corresponding to a per epoch basis of accuracies and losses\n",
    "# 1) Training Accs 2) Training Losses 3) Validation Accuracies 4) Validation Losses\n",
    "\n",
    "def Train(model, numEpochs = epochs, data = training, lossFn = criterion, device = pleaseBeGpu):\n",
    "    \n",
    "    #Lists of training accuracies and losses to return for visualization later\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    \n",
    "    #Lists of validation accuracies and losses to return for visualization later\n",
    "    valAccs = []\n",
    "    valLosses = []\n",
    "    \n",
    "    #Early stopping structures: \n",
    "    stop = EarlyStop(patience) #Create an EarlyStop object that will evaluate and save the best model if patience has been exceeded\n",
    "    areWeThereYet = False #flag for condition of early stopping\n",
    "    topDog = None #Best model to save if early stopping condition has been met\n",
    "    \n",
    "    for epoch in range(numEpochs):\n",
    "        model.train() # turn on batch norm and dropout\n",
    "        numerator = 0\n",
    "        # numBatches = 0 in case want to know how many batches iterating through\n",
    "        totalSamples = 0\n",
    "        totalLoss = 0\n",
    "        for i, (snpBatch, phenotypeBatch) in enumerate(data):\n",
    "            #numBatches += 1 \n",
    "            #print(snpBatch)\n",
    "            #print(phenotypeBatch)\n",
    "            totalSamples += len(phenotypeBatch) #running total of number of samples\n",
    "            \n",
    "            optimizer.zero_grad() #zero the gradient to prevent accumulation; apparently can also do model.zero_grad() as well and does the same thing...\n",
    "            \n",
    "            #move labels to gpu (hopefully)\n",
    "            snpBatch = snpBatch.to(device)\n",
    "            phenotypeBatch = phenotypeBatch.to(device)\n",
    "            \n",
    "            #Run it:\n",
    "            output = model(snpBatch)\n",
    "            loss = lossFn(output[0], phenotypeBatch.squeeze(1)) #recall output returns a list to handle multi-task learning so here the prediction is output[0]\n",
    "            \n",
    "            #Calculate accuracy:\n",
    "            numerator += AccuracyCalc(output[0], phenotypeBatch.squeeze(1))\n",
    "            totalLoss += loss.item()\n",
    "            \n",
    "            #Train it: \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            \n",
    "        epochAcc = numerator/(totalSamples)\n",
    "        print(\"Training accuracy at epoch {} is: {} .\".format(epoch+1,epochAcc))\n",
    "        print(\"Training loss at epoch {} is : {} .\".format(epoch+1, totalLoss))\n",
    "        accuracies.append(epochAcc)\n",
    "        losses.append(totalLoss)\n",
    "        \n",
    "        #Make some sort of call to Val when finished --> can discuss how want to do this of not a happy camper w/ this\n",
    "        valAcc, valLoss = Validate(model, epoch + 1)\n",
    "        valAccs.append(valAcc)\n",
    "        valLosses.append(valLoss)\n",
    "        \n",
    "        #Early Stopping: \n",
    "        areWeThereYet, topDog = stop(valLoss, model)\n",
    "        if areWeThereYet:\n",
    "            #Change this name later so can include hyperparametrs, etc\n",
    "            torch.save(topDog.state_dict(), '{}.pt'.format(\"BestModel_\"+str(epoch) +\"_epochs_\" + str(numLayers) + \"_hiddenLayers\")) \n",
    "            print(\"Early stopping at epoch:\" + str(epoch))\n",
    "            break\n",
    "        \n",
    "        \n",
    "    #Save final model if no early stopping\n",
    "    if not areWeThereYet:\n",
    "        print(\"Ugh didn't get to go home early, well lets save the final epoch model anyway.\")\n",
    "        torch.save(topDog.state_dict(), '{}.pt'.format(\"BestModel_\"+str(epochs) +\"_epochs_\" + str(numLayers) + \"_hiddenLayers\"))\n",
    "    \n",
    "    return accuracies, losses, valAccs, valLosses\n",
    "        \n",
    "      \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with dataloader\n",
    "Train(derekZoolander, numEpochs = epochs, data = train_loader, lossFn = criterion, device = pleaseBeGpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick Test run to ensure skeleton code (should) work(s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = training.to(pleaseBeGpu)\n",
    "trainingLabels = trainingLabels.to(pleaseBeGpu)\n",
    "print(\"Targets are: {}\".format(trainingLabels.squeeze(1)))\n",
    "print(\"\\n\")\n",
    "\n",
    "derekZoolander.train()\n",
    "optimizer.zero_grad()\n",
    "print(\"Summary before simulated training: \\n\")\n",
    "output = derekZoolander(training)\n",
    "loss = criterion(output[0], trainingLabels.squeeze(1)) #recall output returns a list to handle multi-task learning so here the prediction is output[0]\n",
    "            \n",
    "#Calculate accuracy:\n",
    "numerator = AccuracyCalc(output[0], trainingLabels.squeeze(1))\n",
    "print(loss.item())\n",
    "    \n",
    "#Train it: \n",
    "loss.backward()\n",
    "optimizer.step()\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Summary after simulated training: \\n\")\n",
    "output = derekZoolander(training)\n",
    "loss = criterion(output[0], trainingLabels.squeeze(1)) #recall output returns a list to handle multi-task learning so here the prediction is output[0]\n",
    "\n",
    "\n",
    "print(loss.item())\n",
    "\n",
    "#Calculate accuracy:\n",
    "numerator = AccuracyCalc(output[0], trainingLabels.squeeze(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Seems to run properly!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main F(x) (or some sort of cohesive structure) and Plotting: Tbd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make some sort of call plotting later..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
